{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "513845ed-b955-407f-bd4f-41703c1b1cb7",
      "metadata": {
        "id": "513845ed-b955-407f-bd4f-41703c1b1cb7"
      },
      "source": [
        "# LELA 60342 Research Methods in Computational and Corpus Linguistics 2\n",
        "### Week 6Â¶\n",
        "\n",
        "Today we are going to learn about using Pytorch to perform classification with sequence models.\n",
        "\n",
        "The dataset we are going to work with consists of just over 10,000 surnames, labelled with 18 different nationalities. The first tasks will be to learn a classifier that can accurately assign a nationality to previously unseen surnames. To do this we will use an LSTM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/surnames_data.csv"
      ],
      "metadata": {
        "id": "n3m4wC-35-b8"
      },
      "id": "n3m4wC-35-b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read the data into a Pandas dataframe:"
      ],
      "metadata": {
        "id": "kP_QQHszscJB"
      },
      "id": "kP_QQHszscJB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "surnames_df=pd.read_csv(\"surnames_data.csv\")\n",
        "surnames_df"
      ],
      "metadata": {
        "id": "_IT9f0pwsOeh"
      },
      "id": "_IT9f0pwsOeh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use hierachical indexing in Pandas to represent the data as sequences of separate characters"
      ],
      "metadata": {
        "id": "6y-35DGhRNWZ"
      },
      "id": "6y-35DGhRNWZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "surnames_df=pd.read_csv(\"surnames_data.csv\")\n",
        "\n",
        "chars=[]\n",
        "index_1=[]\n",
        "index_2=[]\n",
        "for i,row in surnames_df.iterrows():\n",
        "    chars.extend(list(row.surname))\n",
        "    index_1.extend([i]*len(row.surname))\n",
        "    index_2.extend(range(len(row.surname)))\n",
        "\n",
        "surnames_chars = pd.DataFrame(chars,index=[index_1,index_2])\n",
        "surnames_chars.columns = [\"chars\"]\n",
        "surnames_chars"
      ],
      "metadata": {
        "id": "fwHsPLy56JJF"
      },
      "id": "fwHsPLy56JJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the Pandas function get_dummies to produce one-hot codings of the characters"
      ],
      "metadata": {
        "id": "R9iLfifss0Yo"
      },
      "id": "R9iLfifss0Yo"
    },
    {
      "cell_type": "code",
      "source": [
        "surnames_oh=pd.get_dummies(surnames_chars.chars,dtype=int)\n",
        "surnames_oh"
      ],
      "metadata": {
        "id": "pxLril3Gsuje"
      },
      "id": "pxLril3Gsuje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the nationalities"
      ],
      "metadata": {
        "id": "AFKV89B3tGkb"
      },
      "id": "AFKV89B3tGkb"
    },
    {
      "cell_type": "code",
      "source": [
        "nationalities_oh=pd.get_dummies(surnames_df.nationality,dtype=int)\n",
        "nationalities_oh"
      ],
      "metadata": {
        "id": "PSTAY8bxs92w"
      },
      "id": "PSTAY8bxs92w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then turn these into tensors for input to PyTorch and in particular to an LSTM layer. We want a tensor with the shape [Number_of_names, Number_of_characters_in name, Size_of_alphabet]. \\\n",
        "\n",
        "However the LSTM layer requires that all sequence be of the same length and so we pad our tensors by adding N tensors of zeros of the length of the one hot codings to the beginning of each name. So that the tensor actually has the form [Number_of_names, Number_of_characters_in_the_longest_name, Size_of_alphabet]\n",
        "\n",
        "We do this using the function ZeroPad1d which takes as an argument a tuple with the following entries: padding_left, padding_right, padding_above, padding below.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXS6br9XtQne"
      },
      "id": "TXS6br9XtQne"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "t=torch.ones([5,5,5])\n",
        "print(t[0,:,:])\n",
        "m = nn.ZeroPad1d((0,0,2,0))\n",
        "print(m(t))"
      ],
      "metadata": {
        "id": "k3S5qErrFpyM"
      },
      "id": "k3S5qErrFpyM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# Find the length of the longest name in the data:\n",
        "max_length=max([t[1] for t in surnames_oh.index])\n",
        "# Make an array for the name tensors\n",
        "X = [0] * (max(surnames_oh.index)[0]+1)\n",
        "# Make an array for the label tensors\n",
        "y = [0] * (max(nationalities_oh.index)+1)\n",
        "# Iterate over index of the surnames one-hot data frame. The indices are tuples.\n",
        "for ind in surnames_oh.index:\n",
        "    # Make a tensor from subset of the dataframe for this name/index\n",
        "    s=torch.from_numpy(surnames_oh.loc[ind[0]].values).to(dtype=torch.float)\n",
        "    # Pad the tensor\n",
        "    m = nn.ZeroPad1d((0,0,max_length-len(s),0))\n",
        "    # Add tensors to arrays\n",
        "    X[ind[0]] = m(s).cuda()\n",
        "    y[ind[0]] = torch.from_numpy(nationalities_oh.loc[ind[0]].values).to(dtype=torch.float).cuda()\n",
        "# Combine contents of arrays into a single tensor\n",
        "X=torch.stack(X)\n",
        "y=torch.stack(y)"
      ],
      "metadata": {
        "id": "q_O3VAIvD6Wo"
      },
      "id": "q_O3VAIvD6Wo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "vclVN6WKulAH"
      },
      "id": "vclVN6WKulAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "be8pT09funpG"
      },
      "id": "be8pT09funpG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM layers in PyTorch\n",
        "\n",
        "LSTM layers can be specified as follows. We need to specify the size of the input (e.g. the length our one-hot vectors), the size of the hidden layer to use, the number of layers to include. And because of the way that our data is configured we use the batch_first flag.\n"
      ],
      "metadata": {
        "id": "abr-O-JLuzMz"
      },
      "id": "abr-O-JLuzMz"
    },
    {
      "cell_type": "code",
      "source": [
        "input=torch.randn((1,10,10))\n",
        "lstm = nn.LSTM(input_size=10, hidden_size=5, num_layers=1, batch_first=True)\n",
        "output, (hidden, cell)=lstm(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "jPbVOE8au5sr"
      },
      "id": "jPbVOE8au5sr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is the final hidden layers from each step of the sequence. The second element output is a tuple containing the hidden and cell states from all layers and timepoints. Here we are interested in the hidden layer values as it is the hidden layer from final step for each sequence that we will pass on to a linear layer to perform classification. We could take this from either the output or the hidden objects. In our code we take this from the output."
      ],
      "metadata": {
        "id": "8NuBmdaExxvW"
      },
      "id": "8NuBmdaExxvW"
    },
    {
      "cell_type": "code",
      "source": [
        "hidden.shape"
      ],
      "metadata": {
        "id": "Zc9FU6-XvMEu"
      },
      "id": "Zc9FU6-XvMEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "n_classes = 18\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=84, hidden_size=42, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(42, n_classes)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "41OD6uUR5-lV"
      },
      "id": "41OD6uUR5-lV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the model we can split the data then train and then test. We will use CrossEntropyLoss because our output is an 18-class softmax. We will use batch training."
      ],
      "metadata": {
        "id": "85ddcHHby_5z"
      },
      "id": "85ddcHHby_5z"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"
      ],
      "metadata": {
        "id": "fm2Ccx7Gha94"
      },
      "id": "fm2Ccx7Gha94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import gen_batches\n",
        "import matplotlib.pyplot as plt\n",
        "n_epochs = 150\n",
        "batch_size = 128\n",
        "model = SeqModel()\n",
        "model.to(\"cuda\")\n",
        "ce_loss=[]\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    cumul_loss = 0.0\n",
        "    batches = gen_batches(X_train.shape[0],batch_size)\n",
        "    cumul_loss=0.0\n",
        "    for k in batches:\n",
        "          inputs=X_train[k]\n",
        "          outputs=y_train[k]\n",
        "          y_pred = model(inputs)\n",
        "          loss = loss_fn(y_pred, outputs)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          cumul_loss += loss.item()\n",
        "    ce_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_epochs),ce_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TJEiFBuI5-od"
      },
      "id": "TJEiFBuI5-od",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our accuracy is as follows:"
      ],
      "metadata": {
        "id": "jSqBSq9JzVMH"
      },
      "id": "jSqBSq9JzVMH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_test_pred=[np.argmax(x.cpu().detach().numpy()) for x in model(X_test)]\n",
        "np.mean([int(x == np.argmax(y_test.cpu().detach().numpy()[i])) for i,x in enumerate(y_test_pred)])"
      ],
      "metadata": {
        "id": "1qsKrjfflUdt"
      },
      "id": "1qsKrjfflUdt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try the model out of individual names as follows:"
      ],
      "metadata": {
        "id": "yZ4DTG3WzX8u"
      },
      "id": "yZ4DTG3WzX8u"
    },
    {
      "cell_type": "code",
      "source": [
        "name=\"bannard\"\n",
        "torch.manual_seed(42)\n",
        "charset=list(surnames_oh.columns.values)\n",
        "nationalities=list(nationalities_oh.columns.values)\n",
        "oh = torch.zeros(16,len(charset))\n",
        "for i,c in enumerate(name):\n",
        "    oh[16-len(name)+i,charset.index(c)] = 1.0\n",
        "oh=oh.to(\"cuda\")\n",
        "print(oh.shape)\n",
        "pred=model(torch.unsqueeze(oh,0))\n",
        "nationalities[np.argmax(pred.cpu().detach().numpy())]"
      ],
      "metadata": {
        "id": "36tN7cPXmgMS"
      },
      "id": "36tN7cPXmgMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intent classification with LSTMs\n",
        "\n",
        "Now we are going to work with some sentences - utterances input to a dialogue system assigned with the speaker intent. \\\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\" \\\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" \\\n",
        "'RateBook' e.g. \"give this novel 5 stars\" \\\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\" \\\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\" \\\n",
        "'GetWeather' e.g. \"what's it like outside\" \\\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\" \\\n",
        "\n"
      ],
      "metadata": {
        "id": "GQoPCi6UJxQL"
      },
      "id": "GQoPCi6UJxQL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Build and train an LSTM-based classifier using a training subset of the data that can correctly classify a test subset of the data."
      ],
      "metadata": {
        "id": "fmxSNjClKYod"
      },
      "id": "fmxSNjClKYod"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have prebuilt the tensors containing word embeddings for you:"
      ],
      "metadata": {
        "id": "or-WrAH8KtmW"
      },
      "id": "or-WrAH8KtmW"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/utts_emb.pt.gz\n",
        "!gunzip utts_emb.pt.gz\n",
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/intents_emb.pt\n",
        "X_utts=torch.load(\"utts_emb.pt\")\n",
        "y_intents=torch.load(\"intents_emb.pt\")"
      ],
      "metadata": {
        "id": "BEeT0FQIHStr"
      },
      "id": "BEeT0FQIHStr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_utts.shape"
      ],
      "metadata": {
        "id": "T9dzUOK3H_Lf"
      },
      "id": "T9dzUOK3H_Lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_intents.shape"
      ],
      "metadata": {
        "id": "hvOm7pFUIfUo"
      },
      "id": "hvOm7pFUIfUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_utts, y_intents, test_size=0.2, random_state=30)\n",
        "X_train=X_train.to(\"cuda\")\n",
        "X_test=X_test.to(\"cuda\")\n",
        "y_train=y_train.to(\"cuda\")\n",
        "y_test=y_test.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "7lef6r6KIiWi"
      },
      "id": "7lef6r6KIiWi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained you can calculate its accuracy as follows:\n"
      ],
      "metadata": {
        "id": "bSsfKK-lK_5x"
      },
      "id": "bSsfKK-lK_5x"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_test_pred=[np.argmax(x.cpu().detach().numpy()) for x in model(X_test)]\n",
        "np.mean([int(x == np.argmax(y_test.cpu().detach().numpy()[i])) for i,x in enumerate(y_test_pred)])"
      ],
      "metadata": {
        "id": "3u0KJkuDIiek"
      },
      "id": "3u0KJkuDIiek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: Starting from the Pandas data_frame intent_classification (imported as below), compile the data into the format needed by your model. Note that the utterances are of different lengths so you will need to do some padding. The data frame is hierachically indexed for utterance and word, so that the format is almost identical to the name data. Once you have compiled the data use it to train your model above."
      ],
      "metadata": {
        "id": "Ap-yO1yLLOA5"
      },
      "id": "Ap-yO1yLLOA5"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/intent_classification.pickle"
      ],
      "metadata": {
        "id": "DMA3c9qPNxVc"
      },
      "id": "DMA3c9qPNxVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_classification = pd.read_pickle(\"intent_classification.pickle\")\n"
      ],
      "metadata": {
        "id": "l0hNAdUSN8au"
      },
      "id": "l0hNAdUSN8au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_classification"
      ],
      "metadata": {
        "id": "7-oT4MeQOFGM"
      },
      "id": "7-oT4MeQOFGM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}