{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LELA 60342 Research Methods in Computational and Corpus Linguistics\n",
        "\n",
        "### Week 5\n",
        "\n",
        "Today we will continue to expand our knowledge of PyTorch for implementing and training models."
      ],
      "metadata": {
        "id": "dGnzyofdYbYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of week 2's session you were asked to implement a logistic regression for some simulated data, making use of the backwards step implementation in PyTorch and the appropriate PyTorch loss function. Your code should have looked something like this:"
      ],
      "metadata": {
        "id": "vBfotYi0NLaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V_5ff4IYajA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (2, 3)\n",
        "w2_center = (3, 2)\n",
        "batch_size=50\n",
        "\n",
        "x = np.zeros((batch_size, 2))\n",
        "y = np.zeros(batch_size)\n",
        "for i in range(batch_size):\n",
        "    if np.random.random() > 0.5:\n",
        "        x[i] = np.random.normal(loc=w1_center)\n",
        "    else:\n",
        "        x[i] = np.random.normal(loc=w2_center)\n",
        "        y[i] = 1\n",
        "x=np.insert(x, 2, 1,axis=1)\n",
        "x=torch.tensor(x,dtype=torch.float32)\n",
        "y=torch.tensor(y,dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=3\n",
        "weights = torch.randn(num_features, requires_grad=True)\n",
        "n_iters = 5000\n",
        "num_samples = len(y)\n",
        "lr=0.001\n",
        "logistic_loss=[]\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z=torch.mv(x,weights)\n",
        "    q=torch.sigmoid(z)\n",
        "\n",
        "    loss=loss_fn(q,y)\n",
        "    logistic_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "\n",
        "    dw1 =  weights.grad[0]\n",
        "    dw2 =  weights.grad[1]\n",
        "    db =   weights.grad[2]\n",
        "    with torch.no_grad():\n",
        "      weights[0] = weights[0] - lr * dw1\n",
        "      weights[1] = weights[1] - lr * dw2\n",
        "      weights[2] = weights[2] - lr * db\n",
        "      weights.grad=None\n",
        "\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "RcAMOQF-YyyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we are going to switch to using the full resources of PyTorch in order to implement model training. First it will be useful to learn about a resource in scikit-learn for forming batches:"
      ],
      "metadata": {
        "id": "QphF5lLdN-KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forming batches using scikit-learn\n",
        "\n",
        "As we have discussed previously, model training is often improved by splitting the data up into batches. There is a function in Scikit-learn gen_batches, that is useful to us."
      ],
      "metadata": {
        "id": "Ww9t_KP_2W8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import gen_batches\n",
        "batches = gen_batches(len(x),10)"
      ],
      "metadata": {
        "id": "1sfEFmCX2fCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in batches:\n",
        "     print(x[s])"
      ],
      "metadata": {
        "id": "HQFQafhZ2gOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features=3\n",
        "weights = torch.randn(num_features, requires_grad=True)\n",
        "n_iters = 5000\n",
        "num_samples = len(y)\n",
        "lr=0.001\n",
        "logistic_loss=[]\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "for i in range(n_iters):\n",
        "    batches = gen_batches(len(x),10)\n",
        "    cumul_loss=0.0\n",
        "    for j in batches:\n",
        "        inputs = x[j]\n",
        "        outputs = y[j]\n",
        "        z=torch.mv(inputs,weights)\n",
        "        q=torch.sigmoid(z)\n",
        "\n",
        "        loss=loss_fn(q,outputs)\n",
        "        cumul_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        dw1 =  weights.grad[0]\n",
        "        dw2 =  weights.grad[1]\n",
        "        db =   weights.grad[2]\n",
        "        with torch.no_grad():\n",
        "          weights[0] = weights[0] - lr * dw1\n",
        "          weights[1] = weights[1] - lr * dw2\n",
        "          weights[2] = weights[2] - lr * db\n",
        "          weights.grad=None\n",
        "        cumul_loss += loss.item()/10\n",
        "    logistic_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "SA3WTpE32j4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using torch.nn.Module to define models\n",
        "\n",
        "torcn.nn is a subpackage containing all sorts of tools for working with neural networks. One particularly valuable component is nn.Module which we can use as base class for specifying all of our models"
      ],
      "metadata": {
        "id": "baZioX1Xc4-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class logistic_regression(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(logistic_regression, self).__init__()\n",
        "    self.layer_1 = nn.Linear(3,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return(torch.sigmoid(self.layer_1(x)))"
      ],
      "metadata": {
        "id": "-Pt8nipydChc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using torch.optim\n",
        "\n",
        "\n",
        "So far we have been updating our model weights (optimizing our models) manually. This become fiddly when we work with anything but toy models. Fortunately Pytorch provides a package that implements optimization for us. (https://pytorch.org/docs/stable/optim.html)"
      ],
      "metadata": {
        "id": "R-uM30u-2RnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 1000\n",
        "logistic_loss=[]\n",
        "model = logistic_regression()\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "for i in range(n_iters):\n",
        "    batches = gen_batches(len(x),10)\n",
        "    cumul_loss = 0.0\n",
        "    for j in batches:\n",
        "        inputs=x[j]\n",
        "        labels=y[j]\n",
        "        optimizer.zero_grad()\n",
        "        pred=model(inputs)\n",
        "        loss=loss_fn(torch.squeeze(pred),labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cumul_loss += loss.item()\n",
        "    logistic_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "xyK3glBoehU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using CUDA"
      ],
      "metadata": {
        "id": "GRXDPwLjoFdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 500\n",
        "logistic_loss=[]\n",
        "model = logistic_regression()\n",
        "model.to(\"cuda\")\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "for i in range(n_iters):\n",
        "    batches = gen_batches(len(x),10)\n",
        "    cumul_loss = 0.0\n",
        "    for j in batches:\n",
        "        inputs=x[j].to(\"cuda\")\n",
        "        labels=y[j].to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        pred=model(inputs)\n",
        "        loss=loss_fn(torch.squeeze(pred),labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cumul_loss += loss.item()\n",
        "    logistic_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "qY3mE6CroEkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building and testing a Sentiment Classifier in Pytorch"
      ],
      "metadata": {
        "id": "RYXcpH9HqxUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/yelp_reviews_oh.csv.gz\n",
        "!gunzip yelp_reviews_oh.csv.gz\n",
        "import pandas as pd\n",
        "reviews_df_oh=pd.read_csv(\"yelp_reviews_oh.csv\")\n",
        "reviews_oh=torch.tensor(reviews_df_oh.iloc[:,2:5002].values,dtype=torch.float32)\n",
        "labels=torch.tensor((reviews_df_oh.iloc[:,1] == \"positive\").astype(int),dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "Gdgsfyrf2kbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "reviews_train, reviews_test, labels_train, labels_test = train_test_split(reviews_oh, labels, test_size=0.2, random_state=30)"
      ],
      "metadata": {
        "id": "45xbysfPQ9Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Implement a logistic regression sentiment classifier using Pytorch. Use batch training. Use the GPU via CUDA."
      ],
      "metadata": {
        "id": "SveQB4dxnkQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have trained your model you can calculate its precision and recall on the test data with the following lines:"
      ],
      "metadata": {
        "id": "xtldMfjeUlFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_test=reviews_test.to(\"cuda\")\n",
        "labels_test=labels_test.to(\"cuda\")\n",
        "\n",
        "labels_pred=(model(reviews_test) > 0.5).int().flatten().tolist()\n",
        "TPs=sum([int(s == 1 and labels_test[i].item() == 1) for i,s in enumerate(labels_pred)])\n",
        "FPs=sum([int(s == 1 and labels_test[i].item() == 0) for i,s in enumerate(labels_pred)])\n",
        "FNs=sum([int(s == 0 and labels_test[i].item() == 1) for i,s in enumerate(labels_pred)])\n",
        "precision=TPs/(TPs+FPs)\n",
        "recall=TPs/(TPs+FNs)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "metadata": {
        "id": "qsEVCAHawMoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to check for overfitting it is also sometimes useful to compare performance on the test data to performance on the training data. An overfit model will perform near perfectly on the training data but poorly on the test data."
      ],
      "metadata": {
        "id": "DaZOvPG41XZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train=reviews_train.to(\"cuda\")\n",
        "labels_train=labels_train.to(\"cuda\")\n",
        "\n",
        "train_labels_pred=(model(reviews_train) > 0.5).int().flatten().tolist()\n",
        "TPs=sum([int(s == 1 and labels_train[i].item() == 1) for i,s in enumerate(train_labels_pred)])\n",
        "FPs=sum([int(s == 1 and labels_train[i].item() == 0) for i,s in enumerate(train_labels_pred)])\n",
        "FNs=sum([int(s == 0 and labels_train[i].item() == 1) for i,s in enumerate(train_labels_pred)])\n",
        "precision=TPs/(TPs+FPs)\n",
        "recall=TPs/(TPs+FNs)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "metadata": {
        "id": "pumZZOEJ0stA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building more complex models\n",
        "\n",
        "The great advantage of using Pytorch like this is that you can easily add greater complexity to your model without having to worry about implementing all the fiddly details."
      ],
      "metadata": {
        "id": "k4U9UingQPE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding layers\n",
        "\n",
        "You can add multiple layers to your network. You just need to make sure that the shape of the tensor input to each layer matches the tensor output by the preceding layer"
      ],
      "metadata": {
        "id": "ftLZYsky37xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class multilayer_network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(multilayer_network, self).__init__()\n",
        "    self.layer_1 = nn.Linear(5000,250)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.layer_2 = nn.Linear(250,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer_1(x))\n",
        "    return(torch.sigmoid(self.layer_2(x)))\n"
      ],
      "metadata": {
        "id": "D7vwX3v3VKHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Train the model above on the sentiment classification task. This should require minimal changes over the training code you used for problem 1. Examine its performance on the test data and the training data."
      ],
      "metadata": {
        "id": "1TuhU2z6titY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: Add a second hidden layer to the model above. Train and test this model."
      ],
      "metadata": {
        "id": "qeqjWhSm9-DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "\n",
        "Once your model starts to get bigger, overfitting becomes a substantial problem. One strategy for avoiding overfitting is dropout. This involves randomly dropping neurons from the network for each iteration.  This prevents the network from getting stuck in reliance on any single parameter. To implement dropout in PyTorch we simply set a proportion of neurons that we want to drop out for a given layer at each iteration.\n",
        "\n"
      ],
      "metadata": {
        "id": "VNGgeg2hlCcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class multilayer_network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(multilayer_network, self).__init__()\n",
        "    self.layer_1 = nn.Linear(5000,250)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.8)\n",
        "    self.layer_2 = nn.Linear(250,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer_1(x))\n",
        "    x = self.dropout1(x)\n",
        "    return(torch.sigmoid(self.layer_2(x)))"
      ],
      "metadata": {
        "id": "0hvTA2A4mO_Q"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4. Update the model above so that dropout is applied to both the input and the hidden layer. Train and test the model. Experiment with different dropout values. What impact does this have on performance on the test and the training data?"
      ],
      "metadata": {
        "id": "fxf-YMBsuCsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have finished all these problems, make a start on the programming exercise here:\n",
        "\n",
        "https://livemanchesterac-my.sharepoint.com/:w:/g/personal/colin_bannard_manchester_ac_uk/EVgwZiT4urFCr9J6EELCsdIByg-v0bR7T9Ph3O4GFpwiDA?e=eUzh0l"
      ],
      "metadata": {
        "id": "Ss_876o3uqnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next week we will look at implementing networks for processing sequences (RNNs and Transformers) in Pytorch"
      ],
      "metadata": {
        "id": "6u03QIJIk3Ir"
      }
    }
  ]
}