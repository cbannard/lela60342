{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "513845ed-b955-407f-bd4f-41703c1b1cb7",
      "metadata": {
        "id": "513845ed-b955-407f-bd4f-41703c1b1cb7"
      },
      "source": [
        "# LELA 60342 Research Methods in Computational and Corpus Linguistics 2\n",
        "# Week 7\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7255e5-c04a-41a2-9feb-1c769ba0b1ad",
      "metadata": {
        "id": "2b7255e5-c04a-41a2-9feb-1c769ba0b1ad"
      },
      "source": [
        "# Generating with a recurrent neural network\n",
        "\n",
        "The model structure that we used last week for classifying texts with an LSTM can also be used to generate text. We simply need to change the training data so that instead of predicting a label given a sequence of words as an input, the model is trained to predict the next word.\n",
        "\n",
        "For our example here we will generate sequences of characters (as it keeps the \"vocabulary\" small). The training data will be all words that occur more than 3 times in Crime and Punishment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e509eb-0aee-4505-82e3-7aa64440cc2e",
      "metadata": {
        "id": "a8e509eb-0aee-4505-82e3-7aa64440cc2e"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "c_and_p=c_and_p.lower()\n",
        "c_and_p=re.sub('\\n',' ', c_and_p)\n",
        "c_and_p=re.sub('[^a-z ]','', c_and_p)\n",
        "c_and_p=re.sub(' +', ' ',c_and_p)\n",
        "c_and_p=re.split(\" \", c_and_p)\n",
        "from collections import Counter\n",
        "a=Counter(c_and_p)\n",
        "words=[i for i,v in a.items() if v >= 4]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785fbb58-5a52-4444-9f2b-b35a37ab774f",
      "metadata": {
        "id": "785fbb58-5a52-4444-9f2b-b35a37ab774f"
      },
      "source": [
        "In order to allow us to predict characters with a set length (of 5) we add a sequence of upper case characters to the beginning of the string (we lowercased the words so the characters won't occur inside the real words). \\\n",
        "In order to allow us to predict the ends of words, we add an end of word marker (the letter Z) to our sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25fd0a3-e148-4fe6-afdc-677845c9806a",
      "metadata": {
        "id": "d25fd0a3-e148-4fe6-afdc-677845c9806a"
      },
      "outputs": [],
      "source": [
        "words=[\"ABCDE\" + str.lower(x) + \"Z\" for x in words]\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89eea00-3c7b-4810-bfe6-6c920ff3df89",
      "metadata": {
        "id": "c89eea00-3c7b-4810-bfe6-6c920ff3df89"
      },
      "source": [
        "We then use hierachical indexing in Pandas to represent the data as sequences of separate characters nested inside words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fwHsPLy56JJF",
      "metadata": {
        "id": "fwHsPLy56JJF"
      },
      "outputs": [],
      "source": [
        "chars=[]\n",
        "index_1=[]\n",
        "index_2=[]\n",
        "for i,w in enumerate(words):\n",
        "    chars.extend(list(w))\n",
        "    index_1.extend([i]*len(w))\n",
        "    index_2.extend(range(len(w)))\n",
        "\n",
        "words_as_chars = pd.DataFrame(chars,index=[index_1,index_2])\n",
        "words_as_chars.columns = [\"chars\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f97b9f-49a1-40ca-b59c-5d3e4fc6a1f2",
      "metadata": {
        "id": "c9f97b9f-49a1-40ca-b59c-5d3e4fc6a1f2"
      },
      "outputs": [],
      "source": [
        "words_as_chars.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R9iLfifss0Yo",
      "metadata": {
        "id": "R9iLfifss0Yo"
      },
      "source": [
        "We can then use the Pandas function get_dummies to produce one-hot codings of the characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pxLril3Gsuje",
      "metadata": {
        "id": "pxLril3Gsuje"
      },
      "outputs": [],
      "source": [
        "words_oh=pd.get_dummies(words_as_chars.chars,dtype=int)\n",
        "words_oh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a57e655-9a96-4520-89b3-d807e6376a31",
      "metadata": {
        "id": "6a57e655-9a96-4520-89b3-d807e6376a31"
      },
      "source": [
        "We then convert our dataframe in to 3d tensor of shape [number of words, length of longest word, size of \"vocab\"]. This requires us to pad the tensor for each word that is shorter than the longest word with additional tensors of zeros of length equal to the size of the \"vocab\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q_O3VAIvD6Wo",
      "metadata": {
        "id": "q_O3VAIvD6Wo"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "# Find the length of the longest word in the data:\n",
        "max_length=max([t[1] for t in words_as_chars.index])+1\n",
        "# Make an array for the name tensors\n",
        "X = [0] * (max([t[0] for t in words_as_chars.index])+1)\n",
        "# Iterate over index of the surnames one-hot data frame. The indices are tuples.\n",
        "for ind in words_oh.index:\n",
        "    # Make a tensor from subset of the dataframe for this name/index\n",
        "    s=torch.from_numpy(words_oh.loc[ind[0]].values).to(dtype=torch.float)\n",
        "    # Pad the tensor\n",
        "    m = nn.ZeroPad1d((0,0,max_length-len(s),0))\n",
        "    # Add tensors to arrays\n",
        "    X[ind[0]] = m(s)\n",
        "# Combine contents of arrays into a single tensor\n",
        "#print(X[2648])\n",
        "X=torch.stack(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2550c2b9-3141-49e7-a268-2883b0db19cb",
      "metadata": {
        "id": "2550c2b9-3141-49e7-a268-2883b0db19cb"
      },
      "source": [
        "We then use this 3d tensor into an 2d output tensor y which contains the tensors for all the  character in all of our words in order, and a 3d input vector X which contains the tensor for each the N words that preceed each of the characters in y. N here is the length of the context that we want to use in our predictive model (we set this to be 5).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b2155e-3435-40a7-a678-7a2b3067cfb2",
      "metadata": {
        "id": "c6b2155e-3435-40a7-a678-7a2b3067cfb2"
      },
      "outputs": [],
      "source": [
        "X_1=[]\n",
        "y_1=[]\n",
        "n_gram_length=5\n",
        "for i in range(X.shape[0]):\n",
        "    for j in range(n_gram_length,X[i].shape[0]):\n",
        "          X_1.append(X[i,j-n_gram_length:j-1,:])\n",
        "          y_1.append(X[i,j,:])\n",
        "X=torch.stack(X_1)\n",
        "y=torch.stack(y_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e06376c-9ede-4d87-bfaf-c46051dcdb91",
      "metadata": {
        "id": "5e06376c-9ede-4d87-bfaf-c46051dcdb91"
      },
      "source": [
        "The model and training routine is then identical to that which we used for the classifiers last week. We just need to change the code to reflect the size of the input vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41OD6uUR5-lV",
      "metadata": {
        "id": "41OD6uUR5-lV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "n_chars = 32\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=n_chars, hidden_size=15, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(15, n_chars)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJEiFBuI5-od",
      "metadata": {
        "id": "TJEiFBuI5-od"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import gen_batches\n",
        "import matplotlib.pyplot as plt\n",
        "n_epochs = 250\n",
        "batch_size = 128\n",
        "model = SeqModel()\n",
        "#model.to(\"cuda\")\n",
        "ce_loss=[]\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    cumul_loss = 0.0\n",
        "    batches = gen_batches(X.shape[0],batch_size)\n",
        "    cumul_loss=0.0\n",
        "    for k in batches:\n",
        "          inputs=X[k]\n",
        "          outputs=y[k]\n",
        "          y_pred = model(inputs)\n",
        "          loss = loss_fn(y_pred, outputs)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          cumul_loss += loss.item()\n",
        "    ce_loss.append(cumul_loss)\n",
        "\n",
        "plt.plot(range(1,n_epochs),ce_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77752955-3337-4a6e-86ed-6448cd85276b",
      "metadata": {
        "id": "77752955-3337-4a6e-86ed-6448cd85276b"
      },
      "source": [
        "Once we have this trained model we can use it generate random text as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5kNDGZllpZoD",
      "metadata": {
        "id": "5kNDGZllpZoD"
      },
      "outputs": [],
      "source": [
        "    input=\"ABCDE\"\n",
        "    oh = torch.zeros(7,len(charset))\n",
        "    for i,c in enumerate(input):\n",
        "        oh[i,charset.index(c)] = 1.0\n",
        "    oh=torch.unsqueeze(oh,0)\n",
        "    oh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36tN7cPXmgMS",
      "metadata": {
        "id": "36tN7cPXmgMS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Create a list of all the characters used in the words:\n",
        "charset=list(words_oh.columns.values)\n",
        "# Construct the prompt we want to use. In order to generate full words starting with the first letter, we provide an input equal to the\n",
        "# upper case prefix we appended to our training data words. The first predicted character will then be a character that is likely to occur\n",
        "# in first position\n",
        "input=\"ABCDE\"\n",
        "# We are going to build up our words incrementally within a while loop and so we start by initializing empty strings for the next\n",
        "# character in the sequence at each step, and for the full pseudo-word we are creating\n",
        "pseudo_word=\"\"\n",
        "next_char=\"\"\n",
        "# We want to build up our word by incrementally adding characters until we reach an appropriate point to end. We therefore stop\n",
        "# when the model predicts an end word character Z.\n",
        "while next_char != \"Z\":\n",
        "    # We create a tensor that represents the current state of the sequence. At beginning this is just the prompt. After that it is the last\n",
        "    # five characters of the sequence created so far.\n",
        "    oh = torch.zeros(7,len(charset))\n",
        "    for i,c in enumerate(input):\n",
        "        oh[i,charset.index(c)] = 1.0\n",
        "    oh=torch.unsqueeze(oh,0)\n",
        "    # We pass the tensor representation of the sequence so far into our model and pass the output through a softmax layer to generate\n",
        "    # probabilities for each for each of the characters in our character set.\n",
        "    #pred=model(torch.unsqueeze(oh,0))\n",
        "    pred=model(oh)\n",
        "    pred=nn.functional.softmax(pred,dim=1)\n",
        "    pred=pred.detach().numpy().squeeze()\n",
        "    # We now want to choose our next character. We can do this by randomly choosing a character, with the probability of each character being\n",
        "    # sampled being equal to that output by the softmax. However we make a slight tweak and only sample from the 5 most likely characters.\n",
        "    dict = {'chars': charset, 'probs': pred}\n",
        "    s = pd.DataFrame(dict)\n",
        "    s=s.sort_values(by=\"probs\",ascending=False)[:5]\n",
        "    next_char=np.random.choice(list(s.chars),1,list(s.probs))[0]\n",
        "\n",
        "    # We add the sampled character to our pseudo word\n",
        "    pseudo_word = pseudo_word + next_char\n",
        "    # We create our next input to the model by adding the sampled character to the end of the sequence and remove the first character.\n",
        "    input = input[1:] + next_char\n",
        "print(pseudo_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b84b64-b534-4602-81de-7dfbd02a4f0a",
      "metadata": {
        "id": "09b84b64-b534-4602-81de-7dfbd02a4f0a"
      },
      "source": [
        "# Encoder decoder network\n",
        "\n",
        "A more practically useful application of a recurrent neural network for text generation is the Encoder-Decoder network.\n",
        "\n",
        "We are going to implement a toy machine translation engine. It will take in English words and learn to output the same words in reverse."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n"
      ],
      "metadata": {
        "id": "JcK6ba8N_AJq"
      },
      "id": "JcK6ba8N_AJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "IJsV-Wpv_WHW"
      },
      "id": "IJsV-Wpv_WHW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a55b86-e17e-45a8-b609-f5825b4c5836",
      "metadata": {
        "id": "50a55b86-e17e-45a8-b609-f5825b4c5836"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "c_and_p=c_and_p.lower()\n",
        "c_and_p=re.sub('\\n',' ', c_and_p)\n",
        "c_and_p=re.sub('[^a-z ]','', c_and_p)\n",
        "c_and_p=re.sub(' +', ' ',c_and_p)\n",
        "c_and_p=re.split(\" \", c_and_p)\n",
        "from collections import Counter\n",
        "a=Counter(c_and_p)\n",
        "words=[i for i,v in a.items() if v >=  1 and len(i) == 3]\n",
        "chars=[]\n",
        "chars_rev=[]\n",
        "index_1=[]\n",
        "index_2=[]\n",
        "index_1_rev=[]\n",
        "index_2_rev=[]\n",
        "for i,w in enumerate(words):\n",
        "    chars.extend(list(w))\n",
        "    #chars.append(\"A\")\n",
        "    index_1.extend([i]*(len(w)))\n",
        "    index_2.extend(range(len(w)))\n",
        "    chars_rev.append(\"A\")\n",
        "    chars_rev.extend(list(w)[::-1])\n",
        "    #chars_rev.extend(list(w))\n",
        "    index_1_rev.extend([i]*(len(w)+1))\n",
        "    index_2_rev.extend(range(len(w)+1))\n",
        "\n",
        "words_as_chars = pd.DataFrame(chars,index=[index_1,index_2])\n",
        "words_as_chars.columns = [\"chars\"]\n",
        "words_as_chars_rev = pd.DataFrame(chars_rev,index=[index_1_rev,index_2_rev])\n",
        "words_as_chars_rev.columns = [\"chars\"]\n",
        "words_oh=pd.get_dummies(words_as_chars.chars,dtype=int)\n",
        "words_oh_rev=pd.get_dummies(words_as_chars_rev.chars,dtype=int)\n",
        "\n",
        "# Find the length of the longest word in the data:\n",
        "max_length=max([t[1] for t in words_as_chars.index])+1\n",
        "# Make an array for the name tensors\n",
        "X = [0] * (max([t[0] for t in words_as_chars.index])+1)\n",
        "# Iterate over index of the surnames one-hot data frame. The indices are tuples.\n",
        "for ind in words_oh.index:\n",
        "    # Make a tensor from subset of the dataframe for this name/index\n",
        "    s=torch.from_numpy(words_oh.loc[ind[0]].values).to(dtype=torch.float)\n",
        "    # Pad the tensor\n",
        "    m = nn.ZeroPad1d((0,0,max_length-len(s),0))\n",
        "    # Add tensors to arrays\n",
        "    X[ind[0]] = m(s)\n",
        "# Combine contents of arrays into a single tensor\n",
        "X=torch.stack(X)\n",
        "max_length_rev=max([t[1] for t in words_as_chars_rev.index])+1\n",
        "# Make an array for the name tensors\n",
        "X_rev = [0] * (max([t[0] for t in words_as_chars_rev.index])+1)\n",
        "# Iterate over index of the surnames one-hot data frame. The indices are tuples.\n",
        "for ind in words_oh_rev.index:\n",
        "    # Make a tensor from subset of the dataframe for this name/index\n",
        "    s=torch.from_numpy(words_oh_rev.loc[ind[0]].values).to(dtype=torch.float)\n",
        "    # Pad the tensor\n",
        "    m = nn.ZeroPad1d((0,0,max_length_rev-len(s),0))\n",
        "    # Add tensors to arrays\n",
        "    X_rev[ind[0]] = m(s)[1:]\n",
        "# Combine contents of arrays into a single tensor\n",
        "X_rev=torch.stack(X_rev)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "VB_m6LHLlbGN"
      },
      "id": "VB_m6LHLlbGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059f41cc-f0d8-4fe0-a0e6-801ce5aab133",
      "metadata": {
        "id": "059f41cc-f0d8-4fe0-a0e6-801ce5aab133"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "n_chars_enc = 24\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.linear = nn.Linear(n_chars_enc,n_chars_enc)\n",
        "        self.gru = nn.GRU(input_size=n_chars_enc, hidden_size=16, num_layers=1, batch_first=True)\n",
        "        #self.dropout = nn.Dropout(0.1)\n",
        "    def forward(self, x):\n",
        "        #x=self.dropout(self.linear(x))\n",
        "        x, h = self.gru(x)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5c8162-16b9-4d54-a26f-78c7fa42b3c0",
      "metadata": {
        "id": "4c5c8162-16b9-4d54-a26f-78c7fa42b3c0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "n_chars_dec = 25\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.linear1 = nn.Linear(n_chars_dec, n_chars_dec)\n",
        "        self.gru = nn.GRU(input_size=n_chars_dec, hidden_size=16, num_layers=1, batch_first=True)\n",
        "        self.linear2 = nn.Linear(16, n_chars_dec)\n",
        "    def forward(self, x, hidden_state):\n",
        "        #x=self.linear1(x)\n",
        "        #x = F.relu(x)\n",
        "        x, h  = self.gru(x, hidden_state)\n",
        "        # take only the last output\n",
        "        x = x[-1, :]\n",
        "        x = self.linear2(x)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5edd1cb4-a447-4a93-9344-c6239610f46a",
      "metadata": {
        "id": "5edd1cb4-a447-4a93-9344-c6239610f46a"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Encoder = Encoder()\n",
        "        self.Decoder = Decoder()\n",
        "\n",
        "    def forward(self, x, teach):\n",
        "        y = teach[0]\n",
        "        tfr = teach[1]\n",
        "        outputs= [0] * x.shape[0]\n",
        "        for j,xi in enumerate(x):\n",
        "           teacher_sig = y[j]\n",
        "           h = self.Encoder(xi)\n",
        "           xi = torch.zeros((1,n_chars_dec),device=\"cuda\")\n",
        "           xi[0,0] = 1.0\n",
        "           output = [0] * 3\n",
        "           for i in range(0,3):\n",
        "             xi, h = self.Decoder(xi,h)\n",
        "             xi=torch.unsqueeze(xi,0)\n",
        "             # With a specified probability tfr the model will use the target token as the input from the previous step rather than the model output\n",
        "             if np.random.binomial(1, tfr) == 1:\n",
        "              xi = teacher_sig[i]\n",
        "              xi=torch.unsqueeze(xi,0)\n",
        "             output[i] = xi\n",
        "\n",
        "\n",
        "           output=torch.stack(output)\n",
        "           output=torch.squeeze(output,1)\n",
        "           outputs[j]=output\n",
        "        outputs=torch.stack(outputs)\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26776f9a-4339-4ba0-a3ad-4dfcf0d43f18",
      "metadata": {
        "id": "26776f9a-4339-4ba0-a3ad-4dfcf0d43f18",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import gen_batches\n",
        "import matplotlib.pyplot as plt\n",
        "n_epochs =250\n",
        "batch_size = 52\n",
        "model = Seq2Seq()\n",
        "tfr=0.2\n",
        "\n",
        "ce_loss=[]\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model=model.to(\"cuda\")\n",
        "X=X.to(\"cuda\")\n",
        "X_rev=X_rev.to(\"cuda\")\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print(i)\n",
        "    batches = gen_batches(X.shape[0],batch_size)\n",
        "    cumul_loss=0.0\n",
        "    for batch in batches:\n",
        "        inputs=X[batch]\n",
        "        outputs=X_rev[batch]\n",
        "        y_preds = model(inputs, (outputs, tfr))\n",
        "        loss = loss_fn(y_preds, outputs)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cumul_loss += loss.item()\n",
        "    ce_loss.append(cumul_loss)\n",
        "plt.plot(range(1,n_epochs),ce_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6027075d-7d5c-499e-8dff-68c94ef0e4b6",
      "metadata": {
        "id": "6027075d-7d5c-499e-8dff-68c94ef0e4b6"
      },
      "source": [
        "# Saving and loading models\n",
        "\n",
        "Before we move on to building models I want to introduce something that will be very important as you build and train more complicated models - saving and loading models and models weights.\n",
        "\n",
        "If you have a trained (or untrained or partially trained) model you can save it as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fdfc97-c8d1-4b65-9ed5-f1b8b91901d5",
      "metadata": {
        "id": "f9fdfc97-c8d1-4b65-9ed5-f1b8b91901d5"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"word_reversal_model_0.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c48a4196-efeb-4733-9552-4cda0c748805",
      "metadata": {
        "id": "c48a4196-efeb-4733-9552-4cda0c748805"
      },
      "source": [
        "The model can then be reloaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23509b4-32e4-4a1c-a11f-a7ee07f6dbdb",
      "metadata": {
        "id": "f23509b4-32e4-4a1c-a11f-a7ee07f6dbdb"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"word_reversal_model_0.pth\", weights_only=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd112ff-d0ff-482e-a989-707c4d7734c3",
      "metadata": {
        "id": "cfd112ff-d0ff-482e-a989-707c4d7734c3"
      },
      "source": [
        "If you have a model and you wanted to just save the weights (a more common situation as you will typically have the code for the model itself and simply want to save the trained weights) then you can do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3251b4-0bf5-4beb-9845-6d28b48d6af5",
      "metadata": {
        "id": "9d3251b4-0bf5-4beb-9845-6d28b48d6af5"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"word_reversal_model_0_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce776770-6b14-4ae1-b344-e31e7241a231",
      "metadata": {
        "id": "ce776770-6b14-4ae1-b344-e31e7241a231"
      },
      "source": [
        "You can load the weights and add them to an instance of your model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5eb097-b003-4a6d-88e7-d43826cfec2a",
      "metadata": {
        "id": "1c5eb097-b003-4a6d-88e7-d43826cfec2a"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(PATH, weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give this a try you can download a file with model and weights for the model above here:"
      ],
      "metadata": {
        "id": "tKGvrzyT3lqF"
      },
      "id": "tKGvrzyT3lqF"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/word_reversal_model_0.pth"
      ],
      "metadata": {
        "id": "KY3V_9k_2dJj"
      },
      "id": "KY3V_9k_2dJj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "JUYm4S_UDM9w",
      "metadata": {
        "id": "JUYm4S_UDM9w"
      },
      "source": [
        "Loading third-party models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QmcgH81IDaY7",
      "metadata": {
        "id": "QmcgH81IDaY7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "checkpoint = \"google/mt5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: The code below defines a function that takes in a word and applys the model above to it. It works occasionally. See it you can improve the model."
      ],
      "metadata": {
        "id": "_jKF7XFA4wav"
      },
      "id": "_jKF7XFA4wav"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p0vsNv2Lsfjb",
      "metadata": {
        "id": "p0vsNv2Lsfjb"
      },
      "outputs": [],
      "source": [
        "charset_enc=list(words_oh.columns)\n",
        "charset_dec=list(words_oh_rev.columns)\n",
        "def decode(input):\n",
        "  test_word = torch.zeros((len(input),n_chars_enc),device=\"cuda\")\n",
        "  target_word = torch.zeros((len(input),n_chars_dec),device=\"cuda\")\n",
        "  for i in range(0,len(input)):\n",
        "    test_word[i,charset_enc.index(input[i])] = 1.0\n",
        "    target_word[i,charset_dec.index(input[i])] = 1.0\n",
        "  output=torch.squeeze(model(torch.unsqueeze(test_word,0),(torch.unsqueeze(target_word,0),0)))\n",
        "  decoded_word = \"\"\n",
        "  for j in range(output.shape[0]):\n",
        "    decoded_word += charset_dec[torch.argmax(output[j]).item()]\n",
        "  return decoded_word\n",
        "\n",
        "input=\"cat\"\n",
        "decode(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Update the model above so that it will work with words of a different length."
      ],
      "metadata": {
        "id": "LHOzQq-S5gkC"
      },
      "id": "LHOzQq-S5gkC"
    },
    {
      "cell_type": "markdown",
      "id": "XyTMoVXgOHGv",
      "metadata": {
        "id": "XyTMoVXgOHGv"
      },
      "source": [
        "# Code-breaking with an Encoder-Decoder network\n",
        "\n",
        "Running the next cell will download and import a dataframe containing coded words paired with their uncoded forms. Train a network that learns to translate decode encoded words into their original form. Use this network to decode the next two encoded words:\n",
        "\n",
        "ocpuk \\\n",
        "tudcr \\\n",
        "\n",
        "You will need to prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s1EE0OTJOrub",
      "metadata": {
        "id": "s1EE0OTJOrub"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60342/refs/heads/main/codes.csv\n",
        "codes=pd.read_csv(\"codes.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}