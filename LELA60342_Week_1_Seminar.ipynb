{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LELA 60342 Research Methods in Computational and Corpus Linguistics\n",
        "## Week 1: Tensors and Operations on Tensors in PyTorch"
      ],
      "metadata": {
        "id": "6NRwL7QZL61R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch\n",
        "\n",
        "Pytorch (https://pytorch.org/) is a very powerful library for building neural network / deep learning models that you are going to be using in CL2 this semester and are likely to want to use in your own research. In these RM in CL sessions I am going to introduce you to it and supplement the work you are going to be doing in CL2. We start by importing the library as follows:"
      ],
      "metadata": {
        "id": "g--O8UG42K4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "1Yg9qpAL4FLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors in Pytorch\n",
        "\n",
        "At its heart Pytorch (like all deep learning libraries) is a tool for manipulating tensors. You encountered Tensors in Numpy in CL1 last semester. Much of what you will do with them in Pytorch will seem familiar, but Pytorch adds a few things that make Tensors much more useful to us in Deep Learning. Firstly Pytorch supports very efficent computation with tensors on the GPU. Secondly Pytorch provides a package (autograd) for the automatic calculation of gradients, and these gradients are stored in tensor objects. We will get to that in a bit. First we need to create Tensors."
      ],
      "metadata": {
        "id": "uOPtLXM_jsA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tensors"
      ],
      "metadata": {
        "id": "fjINqotP2fa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create from arrays"
      ],
      "metadata": {
        "id": "3-zX4wRGl2dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,4,5])"
      ],
      "metadata": {
        "id": "LNPvGp6Xl1xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1,2,4,5],[1,2,4,5]])"
      ],
      "metadata": {
        "id": "mgZu-o0al16a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise by size"
      ],
      "metadata": {
        "id": "LeMvndmTpe1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(3,5)"
      ],
      "metadata": {
        "id": "3ivguXsIpk5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(10,2)"
      ],
      "metadata": {
        "id": "vz2t_TfYplJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Randomly populated tensors\n",
        "See here for range of options: \\\\\n",
        "https://pytorch.org/docs/stable/torch.html#random-sampling"
      ],
      "metadata": {
        "id": "YLnTW-eHppvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uniform values from 0 to 1\n",
        "torch.rand(10)"
      ],
      "metadata": {
        "id": "UEmsN4AFpyuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Values from normal distribution with mean of 0 and standard deviation of 1\n",
        "torch.randn(10)"
      ],
      "metadata": {
        "id": "1Tx0Zc9Pp8fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create based on existing tensor"
      ],
      "metadata": {
        "id": "WH7rk1h6Nbe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.zeros(10)\n",
        "torch.full_like(a,11)"
      ],
      "metadata": {
        "id": "3qoArj7JNiWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.full_like(a,0.2)"
      ],
      "metadata": {
        "id": "Bc_v6RdON8Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specifying data type\n",
        "You can specify the data type of your tensor. See here for the full set of PyTorch data types:\n",
        "\n",
        "https://pytorch.org/docs/stable/tensors.html#data-types"
      ],
      "metadata": {
        "id": "vukUXU6b2h84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(10,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "N9x1J3ZdNJtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(10,dtype=torch.float64)"
      ],
      "metadata": {
        "id": "ix1qdUmdOOZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.zeros(10)\n",
        "torch.full_like(a,11,dtype=torch.int32)"
      ],
      "metadata": {
        "id": "KJLtA4DbOUWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running on GPU\n",
        "One of the most valuable things about PyTorch is that it can run computations on the GPU. Tensors that are to be used for such computations must be situated on the GPU. By default new tensors will be created on the CPU. However you can assign the device on which you want a tensor to be situated."
      ],
      "metadata": {
        "id": "7DWSb-Ac2pqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tensor in GPU memory\n",
        "a=torch.zeros(10,device=\"cuda\")"
      ],
      "metadata": {
        "id": "6m5liaFBPDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also move the tensor between processors"
      ],
      "metadata": {
        "id": "LsK9j_LCPRWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.zeros(10)\n",
        "a=a.to(\"cuda\")"
      ],
      "metadata": {
        "id": "hGNh0sxJPVX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=torch.zeros(10,device=\"cuda\")\n",
        "b=b.to(\"cpu\")"
      ],
      "metadata": {
        "id": "Ynhr_spmPbQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for operations to be applied to tensors all the tensors involved must be on the same device"
      ],
      "metadata": {
        "id": "-ui481vaPlEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a*b"
      ],
      "metadata": {
        "id": "y0G1Wf1VPsXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bear in mind that cuda only works on NVIDIA GPUs and is not available on Apple computers. You can use the GPU on Macs but then you need to use \"mps\" not \"cuda\". See here: https://developer.apple.com/metal/pytorch/"
      ],
      "metadata": {
        "id": "pvu7K9TSQRx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make your code moveable between machines you would so well to set the device based on what is available locally as in the following:"
      ],
      "metadata": {
        "id": "Zsh9ColUP4Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "elif torch.mps.is_available(): # if you never use macs you can omit this\n",
        "  device = \"mps\"    # if you never use macs you can omit this\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "metadata": {
        "id": "U56T2KQOQQ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=torch.zeros(10,device=device)\n",
        "b.device"
      ],
      "metadata": {
        "id": "MnOTy6NGQ-cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathematical Operations on Tensors\n",
        "\n",
        "Like Numpy Torch has functions for a wide array of mathematical operations on tensors. If tensors are on the GPU then the operations are performed on the GPU and the result of the operation will also be situated on the GPU. You can find most operations you want. Full list here:\n",
        "\n",
        "https://pytorch.org/docs/stable/torch.html#math-operations"
      ],
      "metadata": {
        "id": "xAKPCXHj2omw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful to check that the function does what you expect as there are some false friends. For example the torch.dot function (unlike its numpy equivalent)only works for 1D tensors (vectors):"
      ],
      "metadata": {
        "id": "rF-df4DUPkaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.rand(3,4)\n",
        "b=torch.rand(4)\n",
        "\n",
        "torch.dot(a,b)"
      ],
      "metadata": {
        "id": "E2ZCsrmSWCKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For matrix-vector dot product there is a separate function torch.mv (https://pytorch.org/docs/stable/generated/torch.mv.html)"
      ],
      "metadata": {
        "id": "TNQE4QkdWtJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.mv(a,b)"
      ],
      "metadata": {
        "id": "itXPty5CWJJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For matrix-matrix dot product there is a separate function torch.mm (https://pytorch.org/docs/stable/generated/torch.mm.html)"
      ],
      "metadata": {
        "id": "FgOVmU5DXIWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To transpose a tensor x in Pytorch you can write x=x.T\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GELKqUYRkUGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Autograd\n",
        "\n",
        "One of the most powerful aspects of PyTorch is its automatic calculation of derivatives/partial derivatives via the chain rule. This makes the performance of the backpropogation of error in neural network models much more straightforward - we don't need to calculate these ourselves which can become a burden in complex models.\n",
        "\n",
        "Remember that the derivative of a function is the sensitivity the output of a function as a result of a change in the input to that function and we use it to change our model weights in a way that decreases loss.\n",
        "\n",
        "If we have some (e.g. loss) function f with some input tensor, then if we call f.backward() Pytorch will augment the tensors that determine the input to that function with their partial derivatives/gradients. In order for this to happen we have to a flag requires_grad=True for the tensor.\n",
        "\n",
        "Here is an example with a very simple function.\n",
        "\n"
      ],
      "metadata": {
        "id": "6YneTEGX2vyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1],dtype=torch.float32,requires_grad=True)\n",
        "f=x*2\n",
        "f.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "id": "BfJkpiLbiErp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or for longer tensors of weights"
      ],
      "metadata": {
        "id": "r1EHeyLFm0PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,1],dtype=torch.float32,requires_grad=True)\n",
        "f=sum(x*2)\n",
        "f.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "id": "q24SjxQXmjdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will work for much more complex functions including composite (chains of) functions. And critically it will work for all the functions we use in neural networks.\n",
        "\n",
        "We will see in future weeks that Pytorch also provides tools to perform updates for us but we can also just use these gradients to update our model in the same way that we used our manually calculated derivatives in CL1."
      ],
      "metadata": {
        "id": "Z_XFjv4VmVv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bear in mind that tensors hold on their gradients and update them. This will be useful in some model types where we want to accumulate gradients over multiple forward passes (e.g. recurrent neural networks which you will encounter soon) but for feedforward networks of the kind that we have been building so far we want to reset the grads to zero after each forward pass. We can do this with the following command:\n",
        "\n",
        "weights.grad=None\n",
        "\n",
        "And if we want to update tensors for which requires_grad=True then we need to turn off the gradient computation by calling torch.no_grad() e.g.:\n",
        "   with torch.no_grad():\n",
        "      w[0] = w[0] - learning_rate * w.grad[0]"
      ],
      "metadata": {
        "id": "pfE9LzQxYd1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: Write code for a linear regression model predicting y from both features in X (data generated below) using Pytorch with autograd to obtain gradients. Print out the learning curve."
      ],
      "metadata": {
        "id": "ncoaLAnW4zf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([[-0.6832,  0.2324, -1.2326, -0.3170,  0.3240, -1.2326, -1.5989,  0.7818,\n",
        "-0.3170,  0.2324,  1.0565,  1.4228,  1.3312],\n",
        "        [-1.5407, -1.2839, -1.0271, -0.7703, -0.5136, -0.2568,  0.0000,  0.2568,\n",
        "          0.5136,  0.7703,  1.0271,  1.2839,  1.5407]])\n",
        "y=torch.tensor([33,49,41,54,52,45,36,58,45,69,55,56,68])"
      ],
      "metadata": {
        "id": "45iGD5lzUAtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x[0], x[1], s=torch.exp(y/10), alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mz625dNI4k1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Rewrite your code from problem 1 so as to use the GPU for all calculations"
      ],
      "metadata": {
        "id": "Q0g2lmqs5CBj"
      }
    }
  ]
}